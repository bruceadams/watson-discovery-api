/* 
 * Discovery
 *
 * The IBM Watson&trade; Discovery Service is a cognitive search and content analytics engine that you can add to applications to identify patterns, trends and actionable insights to drive better decision-making. Securely unify structured and unstructured data with pre-enriched content, and use a simplified query language to eliminate the need for manual filtering of results.
 *
 * OpenAPI spec version: 1.0
 * 
 * Generated by: https://openapi-generator.tech
 */

/// SourceOptionsWebCrawl : Object defining which URL to crawl and how to crawl it.

#[allow(unused_imports)]
use serde_json::Value;

#[derive(Debug, Serialize, Deserialize)]
pub struct SourceOptionsWebCrawl {
  /// The starting URL to crawl.
  #[serde(rename = "url")]
  url: String,
  /// When `true`, crawls of the specified URL are limited to the host part of the **url** field.
  #[serde(rename = "limit_to_starting_hosts")]
  limit_to_starting_hosts: Option<bool>,
  /// The number of concurrent URLs to fetch. `gentle` means one URL is fetched at a time with a delay between each call. `normal` means as many as two URLs are fectched concurrently with a short delay between fetch calls. `aggressive` means that up to ten URLs are fetched concurrently with a short delay between fetch calls.
  #[serde(rename = "crawl_speed")]
  crawl_speed: Option<String>,
  /// When `true`, allows the crawl to interact with HTTPS sites with SSL certificates with untrusted signers.
  #[serde(rename = "allow_untrusted_certificate")]
  allow_untrusted_certificate: Option<bool>,
  /// The maximum number of hops to make from the initial URL. When a page is crawled each link on that page will also be crawled if it is within the **maximum_hops** from the initial URL. The first page crawled is 0 hops, each link crawled from the first page is 1 hop, each link crawled from those pages is 2 hops, and so on.
  #[serde(rename = "maximum_hops")]
  maximum_hops: Option<i32>,
  /// The maximum milliseconds to wait for a response from the web server.
  #[serde(rename = "request_timeout")]
  request_timeout: Option<i32>,
  /// When `true`, the crawler will ignore any `robots.txt` encountered by the crawler. This should only ever be done when crawling a web site the user owns. This must be be set to `true` when a **gateway_id** is specied in the **credentials**.
  #[serde(rename = "override_robots_txt")]
  override_robots_txt: Option<bool>
}

impl SourceOptionsWebCrawl {
  /// Object defining which URL to crawl and how to crawl it.
  pub fn new(url: String) -> SourceOptionsWebCrawl {
    SourceOptionsWebCrawl {
      url: url,
      limit_to_starting_hosts: None,
      crawl_speed: None,
      allow_untrusted_certificate: None,
      maximum_hops: None,
      request_timeout: None,
      override_robots_txt: None
    }
  }

  pub fn set_url(&mut self, url: String) {
    self.url = url;
  }

  pub fn with_url(mut self, url: String) -> SourceOptionsWebCrawl {
    self.url = url;
    self
  }

  pub fn url(&self) -> &String {
    &self.url
  }


  pub fn set_limit_to_starting_hosts(&mut self, limit_to_starting_hosts: bool) {
    self.limit_to_starting_hosts = Some(limit_to_starting_hosts);
  }

  pub fn with_limit_to_starting_hosts(mut self, limit_to_starting_hosts: bool) -> SourceOptionsWebCrawl {
    self.limit_to_starting_hosts = Some(limit_to_starting_hosts);
    self
  }

  pub fn limit_to_starting_hosts(&self) -> Option<&bool> {
    self.limit_to_starting_hosts.as_ref()
  }

  pub fn reset_limit_to_starting_hosts(&mut self) {
    self.limit_to_starting_hosts = None;
  }

  pub fn set_crawl_speed(&mut self, crawl_speed: String) {
    self.crawl_speed = Some(crawl_speed);
  }

  pub fn with_crawl_speed(mut self, crawl_speed: String) -> SourceOptionsWebCrawl {
    self.crawl_speed = Some(crawl_speed);
    self
  }

  pub fn crawl_speed(&self) -> Option<&String> {
    self.crawl_speed.as_ref()
  }

  pub fn reset_crawl_speed(&mut self) {
    self.crawl_speed = None;
  }

  pub fn set_allow_untrusted_certificate(&mut self, allow_untrusted_certificate: bool) {
    self.allow_untrusted_certificate = Some(allow_untrusted_certificate);
  }

  pub fn with_allow_untrusted_certificate(mut self, allow_untrusted_certificate: bool) -> SourceOptionsWebCrawl {
    self.allow_untrusted_certificate = Some(allow_untrusted_certificate);
    self
  }

  pub fn allow_untrusted_certificate(&self) -> Option<&bool> {
    self.allow_untrusted_certificate.as_ref()
  }

  pub fn reset_allow_untrusted_certificate(&mut self) {
    self.allow_untrusted_certificate = None;
  }

  pub fn set_maximum_hops(&mut self, maximum_hops: i32) {
    self.maximum_hops = Some(maximum_hops);
  }

  pub fn with_maximum_hops(mut self, maximum_hops: i32) -> SourceOptionsWebCrawl {
    self.maximum_hops = Some(maximum_hops);
    self
  }

  pub fn maximum_hops(&self) -> Option<&i32> {
    self.maximum_hops.as_ref()
  }

  pub fn reset_maximum_hops(&mut self) {
    self.maximum_hops = None;
  }

  pub fn set_request_timeout(&mut self, request_timeout: i32) {
    self.request_timeout = Some(request_timeout);
  }

  pub fn with_request_timeout(mut self, request_timeout: i32) -> SourceOptionsWebCrawl {
    self.request_timeout = Some(request_timeout);
    self
  }

  pub fn request_timeout(&self) -> Option<&i32> {
    self.request_timeout.as_ref()
  }

  pub fn reset_request_timeout(&mut self) {
    self.request_timeout = None;
  }

  pub fn set_override_robots_txt(&mut self, override_robots_txt: bool) {
    self.override_robots_txt = Some(override_robots_txt);
  }

  pub fn with_override_robots_txt(mut self, override_robots_txt: bool) -> SourceOptionsWebCrawl {
    self.override_robots_txt = Some(override_robots_txt);
    self
  }

  pub fn override_robots_txt(&self) -> Option<&bool> {
    self.override_robots_txt.as_ref()
  }

  pub fn reset_override_robots_txt(&mut self) {
    self.override_robots_txt = None;
  }

}



